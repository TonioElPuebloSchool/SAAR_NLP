{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import the dataframe from the csv file\n",
    "reviews = pd.read_csv('data\\clean_data\\Musical_instruments_reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start pre-proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look for information about the data.  \n",
    "Is there any link to remove, any emjoy, emoticon or any other information that we do not need, or we need to convert?  \n",
    "We will also probably need to remove the stop words, and convert the text to lower case and lemmatize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that takes a review and retreiives the link if it exists\n",
    "def get_link(review):\n",
    "    #print(f\"review: {review}\")\n",
    "    # create a list of words in the review\n",
    "    review_words = review.split()\n",
    "    # create a list of links\n",
    "    links = []\n",
    "    # iterate through the words in the review\n",
    "    for word in review_words:\n",
    "        # if the word starts with http\n",
    "        if word.startswith('http'):\n",
    "            # append the word to the list of links\n",
    "            links.append(word)\n",
    "    # if there are links in the review\n",
    "    if len(links) > 0:\n",
    "        # return the first link\n",
    "        return links[0]\n",
    "    # if there are no links in the review\n",
    "    else:\n",
    "        # return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.amazon.com/LEVYS-LEATHERS-MMGXL-2-5-BRN-STRAP-EXTENDER/dp/B00BH5N91E/ref=sr_1_2?ie=UTF8&qid;=1361565768&sr;=8-2&keywords;=MMGXL-2.5\n",
      "http://www.amazon.com/gp/product/B0018TC7BW/ref=cm_cr_rev_prod_title\n"
     ]
    }
   ],
   "source": [
    "# iterate through the reviews in the dataframe\n",
    "for rev in reviews['review']:\n",
    "    #print(rev)\n",
    "    # get the link from the review\n",
    "    link = get_link(rev)\n",
    "    # if there is a link in the review\n",
    "    if link is not None:\n",
    "        # print the link\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two different links in the data.  \n",
    "Which means the pipeline will have to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a review and returns emojis or emoticons if they exist\n",
    "import emoji\n",
    "EMOJIS = emoji.EMOJI_DATA\n",
    "def extract_emojis(rev):\n",
    "    return ''.join(c for c in rev if c in EMOJIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the reviews in the dataframe\n",
    "for rev in reviews['review']:\n",
    "    #print(rev)\n",
    "    emojis = extract_emojis(rev)\n",
    "    if len(emojis) > 0:\n",
    "        print(f\"emojis: {(emojis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no emojis in the data, so we do not need to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look for emoticons in the reviews\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "def extract_emoticons(rev):\n",
    "    return ''.join(c for c in rev if c in EMOTICONS_EMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the reviews in the dataframe\n",
    "for rev in reviews['review']:\n",
    "    #print(rev)\n",
    "    emots = extract_emoticons(rev)\n",
    "    if len(emots) > 0:\n",
    "        print(f\"emots: {(emots)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have no emoticons in the data neither."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, we will need to remove the punctuation, and convert the text to lower case.  \n",
    "We will also need to remove the stop words, lemmatize the text and check for spell correction.  \n",
    "Lemmatization is important because it will allow us to reduce the number of words in the vocabulary, and therefore the number of features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes URLs from the input text.\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_urls(\"https://www.google.com\") == \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes HTTP tags from the input text.\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, \"html.parser\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_http_tags(\"<p>hello world</p>\") == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects spelling errors in the input text.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_word = spell.correction(word)\n",
    "            corrected_text.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (spell_correction(\"cpasunmot hopfullly it works welld fr whhat I trys to do\")) == \"cpasunmot hopefully it works well for what I try to do\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lower_case(\"Hello World!\") == \"hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from the input text.\n",
    "    \"\"\"\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    translation_table = str.maketrans('', '', PUNCT_TO_REMOVE)\n",
    "    return text.translate(translation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_punctuation(\"Hello, World!\") == \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str,language: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text.\n",
    "    \"\"\"\n",
    "    STOPWORDS = set(stopwords.words(language))\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in STOPWORDS]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_stopwords(\"Hello the World!\", 'english') == \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatizes words in the input text.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lemmatize(\"feet caring\") == \"foot care\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the pipeline for the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the pre-processing will be a python script, we will create the pipeline here but we will not run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Chains all the cleaning functions together using scikit-learn pipelines.\n",
    "    \"\"\"\n",
    "    preprocessing_steps = [\n",
    "        ('lower_case', FunctionTransformer(lower_case)),\n",
    "        ('remove_urls', FunctionTransformer(remove_urls)),\n",
    "        ('remove_http_tags', FunctionTransformer(remove_http_tags)),\n",
    "        ('remove_punctuation', FunctionTransformer(remove_punctuation)),\n",
    "        ('remove_stopwords', FunctionTransformer(lambda x: remove_stopwords(x, 'english'))),\n",
    "        ('lemmatize', FunctionTransformer(lambda x: lemmatize(x))),  # Replace 'lemmatizer' with your lemmatizer object\n",
    "        ('spell_correction', FunctionTransformer(lambda x: spell_correction(x)))  # Replace 'spell' with your SpellChecker object\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "\n",
    "    # Apply the pipeline to the input text\n",
    "    cleaned_text = preprocessing_pipeline.transform([text][0])\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end of the pre-processing will be the creation of a csv file with the pre-processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"data\\clean_data\\Musical_instruments_reviews_clean.csv\", index_col=0)\n",
    "    df[\"cleaned_text\"] = df.text.apply(lambda x: preprocessing_pipeline(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
