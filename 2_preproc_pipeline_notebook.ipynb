{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import the dataframe from the csv file\n",
    "train_data = pd.read_csv('data/raw_data/train.txt', names=['text', 'emotion'], sep=';')\n",
    "test_data = pd.read_csv('data/raw_data/test.txt', names=['text', 'emotion'], sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start pre-proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look for information about the data.  \n",
    "Is there any link to remove, any emjoy, emoticon or any other information that we do not need, or we need to convert?  \n",
    "We will also probably need to remove the stop words, and convert the text to lower case and lemmatize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that takes a review and retreiives the link if it exists\n",
    "def get_link(review):\n",
    "    #print(f\"review: {review}\")\n",
    "    # create a list of words in the review\n",
    "    review_words = review.split()\n",
    "    # create a list of links\n",
    "    links = []\n",
    "    # iterate through the words in the review\n",
    "    for word in review_words:\n",
    "        # if the word starts with http\n",
    "        if word.startswith('http'):\n",
    "            # append the word to the list of links\n",
    "            links.append(word)\n",
    "    # if there are links in the review\n",
    "    if len(links) > 0:\n",
    "        # return the first link\n",
    "        return links[0]\n",
    "    # if there are no links in the review\n",
    "    else:\n",
    "        # return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_links(data, name):\n",
    "    count = 0\n",
    "    for texts in data['text']:\n",
    "        #print(rev)\n",
    "        # get the link\n",
    "        link = get_link(texts)\n",
    "        # if there is a link\n",
    "        if link is not None:\n",
    "            count += 1\n",
    "    print(f\"Number of links in {name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links in train_data: 199\n",
      "Number of links in test_data: 26\n"
     ]
    }
   ],
   "source": [
    "count_links(train_data, 'train_data')\n",
    "count_links(test_data, 'test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a text and returns emojis or emoticons if they exist\n",
    "import emoji\n",
    "EMOJIS = emoji.EMOJI_DATA\n",
    "def extract_emojis(rev):\n",
    "    return ''.join(c for c in rev if c in EMOJIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the text in the dataframe\n",
    "def count_emojis(data, name):\n",
    "    print(f\"Emojis in {name}:\")\n",
    "    for texts in data['text']:\n",
    "        #print(rev)\n",
    "        emojis = extract_emojis(texts)\n",
    "        if len(emojis) > 0:\n",
    "            print(f\"emojis: {(emojis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis in train_data:\n",
      "Emojis in test_data:\n"
     ]
    }
   ],
   "source": [
    "count_emojis(train_data, 'train_data')\n",
    "count_emojis(test_data, 'test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no emojis in the data, so we do not need to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look for emoticons\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "def extract_emoticons(rev):\n",
    "    return ''.join(c for c in rev if c in EMOTICONS_EMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the text in the dataframe\n",
    "def count_emoticons(data, name):\n",
    "    print(f\"Emoticons in {name}:\")\n",
    "    for texts in data['text']:\n",
    "        #print(rev)\n",
    "        emots = extract_emoticons(texts)\n",
    "        if len(emots) > 0:\n",
    "            print(f\"emots: {(emots)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons in train_data:\n",
      "Emoticons in test_data:\n"
     ]
    }
   ],
   "source": [
    "count_emoticons(train_data, 'train_data')\n",
    "count_emoticons(test_data, 'test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have no emoticons in the data neither."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, we will need to remove the punctuation, and convert the text to lower case.  \n",
    "We will also need to remove the stop words, lemmatize the text and check for spell correction.  \n",
    "Lemmatization is important because it will allow us to reduce the number of words in the vocabulary, and therefore the number of features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lower_case(\"Hello World!\") == \"hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text: str) -> str:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (str): text to remove stop words from\n",
    "\n",
    "    Returns:\n",
    "        str: text with stop words removed\n",
    "    \"\"\"\n",
    "    Text=[i for i in str(text).split() if i not in stop_words]\n",
    "    return \" \".join(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_stop_words(\"Hello the World!\") == \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text: str) -> str:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (_type_): text to remove numbers from\n",
    "\n",
    "    Returns:\n",
    "        _type_: text with numbers removed\n",
    "    \"\"\"\n",
    "    text=''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_numbers(\"Hello 123 World!\") == \"Hello  World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"_summary_\n",
    "    \n",
    "    Args:\n",
    "        text (_type_): text to remove punctuations from\n",
    "        \n",
    "    Returns:\n",
    "        _type_: text with punctuations removed\n",
    "    \"\"\"\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text = text.replace('؛',\"\", )\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text =  \" \".join(text.split())\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_punctuations(\"Hello! World!\") == \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"_summary_\n",
    "    \n",
    "    Args:\n",
    "        text (_type_): text to remove urls from\n",
    "        \n",
    "    Returns:\n",
    "        _type_: text with urls removed\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_urls(\"Hello https://www.world.com!\") == \"Hello \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text: str) -> str:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (str): text to lemmatize\n",
    "\n",
    "    Returns:\n",
    "        str: text with lemmatized words\n",
    "    \"\"\"\n",
    "    lemmatizer= WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text=[lemmatizer.lemmatize(y) for y in text]\n",
    "    return \" \" .join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lemmatization(\"Hello foot Worlds!\") == \"Hello foot Worlds!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the pipeline for the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the pre-processing will be a python script, we will create the pipeline here but we will not run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Chains all the cleaning functions together using scikit-learn pipelines.\n",
    "    \"\"\"\n",
    "    preprocessing_steps = [\n",
    "        ('lower_case', FunctionTransformer(lower_case)),\n",
    "        ('remove_stopwords', FunctionTransformer(remove_stop_words)),\n",
    "        ('remove_numbers', FunctionTransformer(remove_numbers)),\n",
    "        ('remove_urls', FunctionTransformer(remove_urls)),\n",
    "        ('remove_punctuation', FunctionTransformer(remove_punctuations)),\n",
    "        ('lemmatization', FunctionTransformer(lemmatization))\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "\n",
    "    # Apply the pipeline to the input text\n",
    "    cleaned_text = preprocessing_pipeline.transform([text][0])\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end of the pre-processing will be the creation of a csv file with the pre-processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"data\\clean_data\\dataset.csv\", index_col=0)\n",
    "    df[\"text\"] = df.text.apply(lambda x: preprocessing_pipeline(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
